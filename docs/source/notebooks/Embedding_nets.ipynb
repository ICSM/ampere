{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Neural Posterior Estimation with Embedding Networks for high-dimensional data\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### The importance of Summary statistics\n",
    "Neural Posterior Estimation relies upon training a neural network to understand the distribution of relationships between model parameters and the types of data they generate.\n",
    "However, when the dataset is very high dimensional (or alternatively, has many features), training this neural network can be very expensive.\n",
    "In such cases, it is common to turn to some *summary statistics* to provide a lower-dimensional representation of the dataset. \n",
    "The neural netowrk can then learn the distribution of data in this subspace, and the comparison between real and simulated data can then be performed there too.\n",
    "This may speed up the inference dramatically, and potentially also make it more reliable (but then again, it also might not, unless the statistics are well crafted). \n",
    "More information on SBI and summary statistics can be found in [the SBI documentation](https://www.mackelab.org/sbi/tutorial/10_crafting_summary_statistics/)\n",
    "\n",
    "### Automatic summary statistics\n",
    "However, for astronomical data, it may be hard to define reasonable summary statistics *a priori*.\n",
    "They are also often high dimensional - in terms of the kinds of data that Ampere typically handles, if you are working with photometry you probably don't need to worry, but most spectra will exhibit these issues.\n",
    "Hence, if we had a way to handle this automatically, it would be great.\n",
    "\n",
    "One way to do this is to introduce another neural network, called an *embedding network*; so-called because it seems to *embed* the high-dimensional data in a lower-dimensional subspace that can represent it with minimal loss.\n",
    "This embedding is then used as a way to automatically determine summary statistics without knowning what to look for ahead of time. \n",
    "There are a range of architectures that can do this, but Fully Connected, Recurrent and Convolutional Neural Networks are common choices.\n",
    "\n",
    "This tutorial will introduce one of the ways you can use this approach in Ampere, and point you to resources that can help you understand other approaches to defining an embedding network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using embeddings in your code\n",
    "\n",
    "Now we will discuss how to actually put this into practice. \n",
    "\n",
    "### Setup\n",
    "\n",
    "First, we have to make a variety of imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import ampere\n",
    "from ampere.data import Spectrum, Photometry\n",
    "from ampere.infer.sbi import SBI_SNPE\n",
    "from ampere.models import Model\n",
    "from spectres import spectres\n",
    "import pyphot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we have to define our model. Since embeddings are most useful for high-dimensional data, we will return to the simple straight-line model defined in the quickstart guide, and use it to create some spectra and photometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASimpleModel(Model):\n",
    "    '''This is a very simple model in which the flux is linear in wavelength.\n",
    "\n",
    "    This model shows you the basics of writing a model for ampere\n",
    "\n",
    "    '''\n",
    "    def __init__(self, wavelengths, flatprior=True,\n",
    "                 lims=np.array([[-10, 10],\n",
    "                                [-10, 10]])):\n",
    "        '''The model constructor, which will set everything up\n",
    "\n",
    "        This method does essential setup actions, primarily things that \n",
    "        may change from one fit to another, but will stay constant throughout \n",
    "        the fit. This may be things like the grid of wavelengths to calculate\n",
    "        the model output on, or establishing the dust opacities if involved.\n",
    "        There are also several important variables it *MUST* define here\n",
    "        '''\n",
    "        self.wavelength = wavelengths\n",
    "        self.npars = 2 #Number of free parameters for the model (__call__()). For some \n",
    "        # models this can be determined through introspection, but it is still strongly \n",
    "        # recommended to define this explicitly here. Introspection will only be \n",
    "        # attempted if self.npars is not defined.\n",
    "        self.npars_ptform = 2 #Sometimes the number of free parameters is different \n",
    "        # when using the prior transform instead of the prior. In that case, \n",
    "        # self.npars_ptform should also be defined.\n",
    "\n",
    "        # You can do any other set up you need in this method.\n",
    "        # For example, we could define some cases to set up different priors\n",
    "        # But that's for a slightly more complex example.\n",
    "        # Here we'll just use a simple flat prior\n",
    "        self.lims = lims\n",
    "        self.flatprior = flatprior\n",
    "        self.parLabels = [\"slope\", \"intercept\"]\n",
    "\n",
    "    def __call__(self, slope, intercept, **kwargs):\n",
    "        '''The model itself, using the callable class functionality of python.\n",
    "\n",
    "        This is an essential method. It should do any steps required to \n",
    "        calculate the output fluxes. Once done, it should stop the output fluxes\n",
    "        in self.modelFlux.\n",
    "        '''\n",
    "\n",
    "        self.modelFlux = slope*self.wavelength + intercept\n",
    "        return {\"spectrum\": {\"wavelength\": self.wavelength, \"flux\": self.modelFlux}}\n",
    "\n",
    "    def lnprior(self, theta, **kwargs):\n",
    "        \"\"\"The model prior probability distribution\n",
    "       \n",
    "        The prior is essential to most types of inference with ampere. The\n",
    "        prior describes the relative weights (or probabilities if normalised)\n",
    "        of different parameter combinations. Using a normalised prior with\n",
    "        SNPE is strongly recommended, otherwise ampere will attempt\n",
    "        approximate normalisation using Monte Carlo integration.\n",
    "        \"\"\"\n",
    "        if not self.flatprior:\n",
    "            raise NotImplementedError()\n",
    "        slope = theta[0]\n",
    "        #print(slope)\n",
    "        intercept = theta[1]\n",
    "        return (\n",
    "            0\n",
    "            if self.lims[0, 0] < slope < self.lims[0, 1]\n",
    "            and self.lims[1, 0] < intercept < self.lims[1, 1]\n",
    "            else -np.inf\n",
    "        )\n",
    "        \n",
    "\n",
    "    def prior_transform(self, u, **kwargs):\n",
    "        '''The prior transform, which takes samples from the Uniform(0,1)\n",
    "        distribution to the desired distribution.\n",
    "\n",
    "        Prior transforms are essential for SNPE. SNPE needs to be able to\n",
    "        generate samples from the prior, and this method is integral to doing \n",
    "        so. Therefore, unlike other inference methods, if you want to use \n",
    "        SNPE (or other SBI approaches) you need to define *both* lnprior and \n",
    "        prior_transform.\n",
    "        '''\n",
    "        if self.flatprior:\n",
    "            return (self.lims[:,1] - self.lims[:,0]) * u + self.lims[:,0]\n",
    "        else:\n",
    "            raise NotImplementedError()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating synthetic data\n",
    "\n",
    "Now that we have our model, we can generate some ground-truth synthetic observations to test the approach with. Firstly, we compute the model for fixed values of the parameters, with the slope and intercept both set to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavelengths = 10**np.linspace(0., 1.9, 2000)\n",
    "\n",
    "\"\"\" Choose some model parameters \"\"\"\n",
    "slope = 1.  # Keep it super simple for now\n",
    "intercept = 1.\n",
    "\n",
    "# Now init the model:\n",
    "model = ASimpleModel(wavelengths)\n",
    "# And call it to produce the fluxes for our chosen parameters\n",
    "model_dict = model(slope, intercept)\n",
    "model_flux = model_dict['spectrum']['flux']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a noise-free spectrum, we can convolve it with some filters to extract synthetic photometry. In this case, we will use the WISE W1 and *Spitzer* MIPS 70 filters. Then we will add some Gaussian noise to it and create a `Photometry` object for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filterName = np.array(['WISE_RSR_W1', 'SPITZER_MIPS_70'])  \n",
    "\n",
    "libDir = ampere.__file__.strip('__init__.py') # '/home/peter/pythonlibs/ampere/ampere/'\n",
    "libname = f'{libDir}ampere_allfilters.hd5'\n",
    "filterLibrary = pyphot.get_library(fname=libname)\n",
    "filters = filterLibrary.load_filters(filterName, interp=True, \n",
    "                                        lamb=wavelengths*pyphot.unit['micron'])\n",
    "# Now we need to extract the photometry with pyphot\n",
    "# first we need to convert the flux from Fnu to Flambda\n",
    "flam = model_flux / wavelengths**2\n",
    "modSed = []\n",
    "for i, f in enumerate(filters):\n",
    "    lp = f.lpivot.to(\"micron\").value\n",
    "    fphot = f.get_flux(wavelengths*pyphot.unit['micron'], flam*pyphot.unit['flam'], axis=-1).value\n",
    "    modSed.append(fphot*lp**2)\n",
    "\n",
    "modSed = np.array(modSed)\n",
    "\n",
    "input_noise_phot = 0.1  # Fractional uncertainty\n",
    "photunc = input_noise_phot * modSed  # Absolute uncertainty\n",
    "# Now perturb data by drawing from a Gaussian distribution\n",
    "modSed = modSed + np.random.randn(len(filterName)) * photunc \n",
    "\n",
    "photometry = Photometry(filterName=filterName, value=modSed, \n",
    "                        uncertainty=photunc, photunits=\"Jy\", \n",
    "                        libName=libname)\n",
    "# print(photometry.filterMask)\n",
    "photometry.reloadFilters(wavelengths)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we repeat the process for some spectra. We choose to create a synthetic *Spizter*/IRS spectrum, and set it to use fast resampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir = f'{os.getcwd()}/test_data/'\n",
    "specFileExample = 'cassis_yaaar_spcfw_14191360t.fits'\n",
    "irsEx = Spectrum.fromFile(os.path.normpath(dataDir+specFileExample),\n",
    "                            format='SPITZER-YAAAR')\n",
    "spec0 = spectres(irsEx[0].wavelength,wavelengths,model_flux)\n",
    "spec1 = spectres(irsEx[1].wavelength,wavelengths,model_flux)\n",
    "\n",
    "# And again, add some noise to it\n",
    "input_noise_spec = 0.1\n",
    "unc0 = input_noise_spec*spec0\n",
    "unc1 = input_noise_spec*spec1\n",
    "spec0 = spec0 + np.random.randn(len(spec0))*unc0\n",
    "spec1 = spec1 + np.random.randn(len(spec1))*unc1\n",
    "\n",
    "spec0 = Spectrum(irsEx[0].wavelength, spec0, unc0, \"um\", \"Jy\", \n",
    "                    calUnc=0.0025, \n",
    "                    scaleLengthPrior=0.01)  # , resampleMethod=resmethod)\n",
    "spec1 = Spectrum(irsEx[1].wavelength, spec1, unc1, \"um\", \"Jy\", \n",
    "                    calUnc=0.0025, \n",
    "                    scaleLengthPrior=0.01)  # , resampleMethod=resmethod)\n",
    "\n",
    "# Now let's try changing the resampling method so it's faster\n",
    "# This model is very simple so exact flux conservation is not important\n",
    "resmethod = \"fast\"  # \"exact\"#\"fast\"#\n",
    "spec0.setResampler(resampleMethod=resmethod)\n",
    "spec1.setResampler(resampleMethod=resmethod)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we combine our synthetic data into a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [photometry,\n",
    "           # spec0, #Fitting spectra is slow because it needs to do a lot of resampling\n",
    "           spec1   # As a result, we're leaving some of them out\n",
    "           ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining an embedding network and doing inference\n",
    "\n",
    "Now that we have our model and (synthetic) data, the last thing we need before we can do inference is our embedding. \n",
    "SBI is all uses torch, so we need to define our embedding as a torch neural network - or at least one that torch understands. \n",
    "We *could* do that from scratch, with something like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor, nn\n",
    "\n",
    "\n",
    "class FullyCOnnectedNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        output_dim = 20,\n",
    "        num_layers = 3,\n",
    "        num_hidden_units = 100,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        layers = [nn.Linear(input_dim, num_hiddens), nn.ReLU()]\n",
    "        for _ in range(num_layers - 2):\n",
    "            layers.append(nn.Linear(num_hiddens, num_hidden_units))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(num_hiddens, output_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "embedding = FullyCOnnectedNetwork(np.sum([data.wavelength.shape[0] for data in dataset]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find some further information on defining embedding networks in the SBI documentation at https://www.mackelab.org/sbi/tutorial/05_embedding_net/.\n",
    "\n",
    "However, SBI also packages a small selection of pre-defined networks that you can use, and we can specify the same network by just importing one of those and passing it the same setup, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.neural_nets.embedding_nets import FCEmbedding\n",
    "\n",
    "embedding = FCEmbedding(np.sum([data.wavelength.shape[0] for data in dataset]), output_dim=20, num_layers=3, num_hidden_units=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is already a lot easier, at least as long as one of the architectures provided by SBI (Fully Connected and Convolutional Neural Networks) is suitable for your problem.\n",
    "\n",
    "But to try and make life even easier, Ampere can construct an embedding for you, using the architectures packaged by SBI!\n",
    "You can simply accept the default hyperparameters for these networks by passing either `embedding_net='CNN'` or `embedding_net='FC'` when instantiating the inference object, or you can pass Ampere a dictionaary defining the values of the hyperparameters that you want to change along with an indication of the type of network you want to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = {'type': 'FC', 'num_hiddens': 100, 'n_layers': 3, \"output_dim\": 20}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now things proceed in a very similar fashion to a normal NPE run! \n",
    "The only difference is that our dictionary telling Ampere how to define the embedding is passed along with the model and dataset when instantiating our inference object. \n",
    "The dict could also just be replaced with a user-defined network or the strings indicated above for default hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SBI_SNPE(model=model, data=dataset, embedding_net=embedding)\n",
    "\n",
    "# Then we tell it to explore the parameter space\n",
    "optimizer.optimise(nsamples=10000, nsamples_post=10000, n_rounds=1\n",
    "                    )\n",
    "\n",
    "# now we call the postprocessing to produce some figures\n",
    "optimizer.postProcess()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ampere_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
